{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5608b50b",
   "metadata": {},
   "source": [
    "# Applying the trained classifier to a large datasets\n",
    "\n",
    "- Workshop: **Tutorial: High performance computing with Python and RS-DAT, EO summer school 2025**\n",
    "\n",
    "- Date: September 3, 2025\n",
    "\n",
    "\n",
    "In this notebook, we apply a random forest classifier that we have trained in [the previous notebook](./step1_train_on_cutout.ipynb) on a small cutout, to a large Sentinel-2 mosaic, to predict a water mask across the entire Area of Interest (AoI). We use [Dask](https://www.dask.org/) to distribute the prediction process across multiple chunks of the image, which allows us to handle large images efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import geopandas\n",
    "import joblib  # to save and load the model\n",
    "import rioxarray\n",
    "import shapely\n",
    "import xarray as xr\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, Lock\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3baa2-c382-4866-bf2a-8dd2c9c5f4f2",
   "metadata": {},
   "source": [
    "---\n",
    "## Data used in this notebookÂ¶\n",
    "\n",
    "The data used in this workshop can be found on Zenodo ([link](https://zenodo.org/records/16941613)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8bb3d-5867-4809-92ba-339c352e94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to retrive the dataset\n",
    "# !wget https://zenodo.org/api/records/16941613/files-archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab110508-f223-4792-85a1-125896f3eaab",
   "metadata": {},
   "source": [
    "Not that on the [Spider](https://doc.spider.surfsara.nl/en/latest/) platform, which we use for demonstraiton, the data is already available under the following path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5216cb8-c5d1-4bac-96be-974d235fdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/project/remotesensing/Data/eo-summer-school\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6e9ade-5580-4559-9452-819a9565fe26",
   "metadata": {},
   "source": [
    "The input data include:\n",
    "- `sentinel2_rgb_mosaic.tif`: Sentinel-2 RGB mosaic representing the area of interest at 10 m resolution.\n",
    "- `waterbody_labels.gpkg`: manually-created polygons with label values, where label 1 represents water bodies and label 0 represents non-water bodies.\n",
    "\n",
    "In [a previous notebook](./step1_train_on_cutout.ipynb), we have used the label data to train the binary classifier. In this notebook, we apply the trained model to the full extent of the Sentinel-2 mosaic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc91eb",
   "metadata": {},
   "source": [
    "## Overview of the Area of Interest (AoI)\n",
    "\n",
    "First, let's take another look at the AoI by visualizing the Cloud-Optimized GeoTIFF (COG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize an overview of the full RGB\n",
    "path_rgb = data_dir / \"sentinel2_rgb_mosaic.tif\"\n",
    "rgb = rioxarray.open_rasterio(path_rgb, overview_level=1)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "rgb.plot.imshow(ax=ax, robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db23fa",
   "metadata": {},
   "source": [
    "## Loading the dataset and the trained model\n",
    "\n",
    "In the following steps, we will load the full RGB image chunk-wise. Depending on the computation infrastructure, we can adjust the chunk size accordingly. With the `chunks` input argument in `rioxarray.open_rasterio`, we can \"lazily\" load the large image, i.e. create a task graph for loading the data in chunks. The image loading will only take place when we actually need the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure chunksize for the later processing\n",
    "CHUNKSIZE = 4096\n",
    "\n",
    "# Lazy loading of the RGB data, chunked\n",
    "rgb = rioxarray.open_rasterio(path_rgb, chunks={\"band\": -1, \"x\": CHUNKSIZE, \"y\": CHUNKSIZE})\n",
    "rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47445ed",
   "metadata": {},
   "source": [
    "We then load the trained classifier from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6176c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = joblib.load('binary_classifier_waterbody.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c901aa8",
   "metadata": {},
   "source": [
    "## Parallelizing inference on large datasets\n",
    "\n",
    "We setup a Dask cluster, i.e. a pool of \"workers\" connected to a \"scheduler\" that distribute and orchestrate the tasks of the graph. When running locally, one can set up a `LocalCluster` to distribute tasks to threads and/or processes. On a HPC infrastructure running SLURM, one can use Dask Jobqueue's `SLURMCluster` to start a Dask cluster backed by the SLURM scheduler, which provides resources upon request. \n",
    "\n",
    " One can inspect the process the status of the cluster through the Dask dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09452964-a382-48e3-9107-d008c6296912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP A DISTRIBUTED CLUSTER HERE ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255129fc-58a3-4518-8933-e378dccef9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... OR USE LOCAL CLUSTER\n",
    "# cluster = LocalCluster()\n",
    "# client = Client(cluster)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89785b7",
   "metadata": {},
   "source": [
    "Now we will apply the trained classifier to the full image in chunks. Many Python libraries, such as `numpy` functions, can interface to Dask arrays, in which case we can directly pass Dask arrays as input. \n",
    "\n",
    "However, this is not the case for the `predict` function of `sklearn` classifiers. Below we demonstrate how to use xarray's `apply_ufunc()` function to \"map\" the classifier predict function to each chunk of the image. In the section commented out below, we show an alternative approach using xarray's `map_blocks()`, which is more suitable for functions that consume xarray objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5604ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chunk(x):\n",
    "    \"\"\"\n",
    "    Run inference for one chunk of the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input raster bands, with shape (NY, NX, 3)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predictions, with shape (NY, NX)\n",
    "    \"\"\"\n",
    "    *chunk_shape, _ = x.shape\n",
    "    x_reshaped = x.reshape(-1, 3)\n",
    "    y = classifier.predict(x_reshaped) # binary prediction\n",
    "    return y.reshape(chunk_shape)\n",
    "\n",
    "# Map the `classifier.predict` to all chunks of the dataset\n",
    "predictions = xr.apply_ufunc(\n",
    "    predict_chunk,\n",
    "    rgb,\n",
    "    exclude_dims={\"band\"},\n",
    "    input_core_dims=[[\"band\"]],\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[\"uint16\"],\n",
    ")\n",
    "predictions = predictions.rio.write_crs(rgb.rio.crs)  # Make sure CRS is kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a03e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Below is an example predict with xarray.map_blocks\n",
    "# # Prediction on each band\n",
    "# def predict_chunk(chunk, classifier):\n",
    "#     \"\"\"Predict on a chunk of data\"\"\"\n",
    "#     # chunk is now an xarray DataArray\n",
    "#     original_shape = chunk.shape\n",
    "#     reshaped = chunk.data.reshape((chunk.shape[0], -1)).T\n",
    "\n",
    "#     # Predict probabilities\n",
    "#     pred = classifier.predict(reshaped)\n",
    "\n",
    "#     # Reshape back to spatial dimensions with probability classes\n",
    "#     result = pred.T.reshape((1, original_shape[1], original_shape[2]))\n",
    "\n",
    "#     # Return as xarray DataArray with proper coordinates\n",
    "#     return xr.DataArray(\n",
    "#         result,\n",
    "#         dims=['band', 'y', 'x'],\n",
    "#         coords={\n",
    "#             'band': [0],\n",
    "#             'y': chunk['y'],\n",
    "#             'x': chunk['x']\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# # Apply prediction function using xarray.map_blocks\n",
    "# predictions = xr.map_blocks(\n",
    "#     predict_chunk,\n",
    "#     rgb,\n",
    "#     args=[classifier],\n",
    "#     template=xr.DataArray(\n",
    "#         da.zeros((1, rgb.sizes['y'], rgb.sizes['x']), chunks=(-1, CHUNKSIZE, CHUNKSIZE)),\n",
    "#         dims=['band', 'y', 'x'],\n",
    "#         coords={\n",
    "#             'band': [0],\n",
    "#             'y': rgb['y'],\n",
    "#             'x': rgb['x']\n",
    "#         }\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec524c",
   "metadata": {},
   "source": [
    "One can also visualize the task graph of the prediction process using Dask's `visualize` function (note that this requires the `graphviz` package to be installed in the environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8933dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b0660",
   "metadata": {},
   "source": [
    "## Save predictions\n",
    "\n",
    "Now the `predictions` variable has not been computed yet - it still consists of a task graph represented by a Dask array. One can call its `.compute()` method to evaluate the result, and collect it in memory - if the \"client\" memory capacity allows for it. \n",
    "\n",
    "Alternatively, one can instruct the Dask cluster to save the output directly to a (set of) file(s). This is desirable for datasets that do not fit the memory of a single machine.\n",
    "\n",
    "In this example, we will save the predictions to a tiled GeoTIFF file (writing a COG in parallel is tricky). Another suitable file format for (multi-dimensional) array data is [Zarr](https://zarr.dev/), which by was designed for parallel read/write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da5e8f4-3c9b-4a77-aa69-715c8b3b36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path.home() / \"predictions_waterbody.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e46ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a tiled GeoTIFF\n",
    "predictions.rio.to_raster(\n",
    "    output_path,\n",
    "    tiled=True,\n",
    "    BLOCKXSIZE=CHUNKSIZE,\n",
    "    BLOCKYSIZE=CHUNKSIZE,\n",
    "    COMPRESS=\"LZW\",\n",
    "    lock=Lock(\"rio\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1269d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large datasets and multiple dimensions, Zarr is a good alternative to (CO)GeoTIFF\n",
    "# predictions.to_zarr(\n",
    "#     \"predictions_waterbody_full.zarr\",\n",
    "#     mode=\"w\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200c40b",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "\n",
    "Now we can visualize the computed predictions - let's zoom in into a different area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved predictions with overview level\n",
    "predictions_loaded = rioxarray.open_rasterio(output_path, chunks=\"auto\")\n",
    "predictions_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ccf0f-097b-4e79-9779-73bcb459ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounding box\n",
    "bbox = geopandas.GeoSeries(\n",
    "        shapely.box(5.64, 52.41, 6.26, 52.79), # xmin, ymin, xmax, ymax\n",
    "        crs=\"EPSG:4326\") \\\n",
    "    .to_crs(predictions_loaded.rio.crs) \\\n",
    "    .total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66515b-5a88-4eab-9416-50ba5a8882c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_loaded_cutout = predictions_loaded.rio.clip_box(*bbox)\n",
    "rgb_cutout = rgb.rio.clip_box(*bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cutout predictions\n",
    "fig, ax = plt.subplots()\n",
    "rgb_cutout.plot.imshow(ax=ax, alpha=0.6, robust=True)\n",
    "extent = bbox[[0, 2, 1, 3]] # xmin, xmax, ymin, ymax\n",
    "ax.imshow(predictions_loaded_cutout.data.squeeze(), cmap='Blues', alpha=0.8, extent=extent)\n",
    "ax.set_title('Waterbody Mask')\n",
    "ax.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
